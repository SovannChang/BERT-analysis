{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1879363",
   "metadata": {},
   "source": [
    "This document is a demonstration of the BERT model, retrieved from https://huggingface.co/bert-base-cased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe02dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the line below to install the transformers package\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30fb5977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cf0d56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Import BERT (cased)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Pre-built for masking tasks\n",
    "unmasker = pipeline('fill-mask', model='bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419749e7",
   "metadata": {},
   "source": [
    "Testing the \"unmasker\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bb3e1640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well                      0.683454\n",
      "effectively               0.079846\n",
      "efficiently               0.036923\n",
      "far                       0.014120\n",
      "accurately                0.013984\n"
     ]
    }
   ],
   "source": [
    "for dictionary in unmasker(\"This is a test of how [MASK] the model works\"):\n",
    "    print(\"{:<25s} {:f}\".format(dictionary.get(\"token_str\"), dictionary.get(\"score\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cbbd0463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alternative               0.079788\n",
      "other                     0.066093\n",
      "obvious                   0.056552\n",
      "clear                     0.044919\n",
      "possible                  0.040370\n"
     ]
    }
   ],
   "source": [
    "for dictionary in unmasker(\"This is a more ambiguous answer. There is no [MASK] solution\"):\n",
    "    print(\"{:<25s} {:f}\".format(dictionary.get(\"token_str\"), dictionary.get(\"score\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ba226cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",                         0.116117\n",
      "and                       0.051120\n",
      ":                         0.041306\n",
      "-                         0.040721\n",
      "of                        0.022506\n"
     ]
    }
   ],
   "source": [
    "for dictionary in unmasker(\"Gibberish: cardboard indigo dog outer [MASK] turpentine France shellack\"):\n",
    "    print(\"{:<25s} {:f}\".format(dictionary.get(\"token_str\"), dictionary.get(\"score\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5670e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bert_nsp(first_sentence, possible_next_sentence):\n",
    "    encoding = tokenizer(first_sentence, possible_next_sentence, return_tensors=\"pt\")\n",
    "    logits = model(**encoding, labels=torch.LongTensor([1])).logits    # logits are output\n",
    "    \n",
    "    probs = softmax(logits, dim=1)\n",
    "    \n",
    "    print(probs[0,0].item())\n",
    "    \n",
    "    if probs[0,0] < probs[0,1]:\n",
    "        print(\"Not likely to be the next sentence.\")\n",
    "    else:\n",
    "        print(\"Likely to be the next sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "573dcd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999825954437256\n",
      "Likely to be the next sentence.\n"
     ]
    }
   ],
   "source": [
    "run_bert_nsp(\"Folk in those stories had lots of chances of turning back, only they didn't.\", \n",
    "             \"They kept going because they were holding onto something.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "97470bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012897348497062922\n",
      "Not likely to be the next sentence.\n"
     ]
    }
   ],
   "source": [
    "run_bert_nsp(\"I don't like sand.\", \n",
    "             \"You're my favortie deputy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cf865336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8595752120018005\n",
      "Likely to be the next sentence.\n"
     ]
    }
   ],
   "source": [
    "run_bert_nsp(\"I don't like sand\", \n",
    "             \"You're my favortie deputy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "743c0cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997171759605408\n",
      "Likely to be the next sentence.\n"
     ]
    }
   ],
   "source": [
    "run_bert_nsp(\"It's over, Anakin!\", \n",
    "             \"I have the high ground.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "99c41b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4672694504261017\n",
      "Not likely to be the next sentence.\n"
     ]
    }
   ],
   "source": [
    "run_bert_nsp(\"The waves crashed against the shore, leaving a line of foam in their wake.\", \n",
    "             \"The painter applied the final brushstrokes to the canvas, completing the masterpiece.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
